{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron, MNIST\n",
    "---\n",
    "In this notebook, we will train an MLP to classify images from the [MNIST database](http://yann.lecun.com/exdb/mnist/) hand-written digit database.\n",
    "\n",
    "The process will be broken down into the following steps:\n",
    ">1. Load and visualize the data\n",
    "2. Define a neural network\n",
    "3. Train the model\n",
    "4. Evaluate the performance of our trained model on a test dataset!\n",
    "\n",
    "Before we begin, we have to import the necessary libraries for working with data and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and Visualize the [Data](http://pytorch.org/docs/stable/torchvision/datasets.html)\n",
    "\n",
    "Downloading may take a few moments, and you should see your progress as the data is loading. You may also choose to change the `batch_size` if you want to load more data at a time.\n",
    "\n",
    "This cell will create DataLoaders for each of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 10\n",
    "\n",
    "# choose the training and test datasets\n",
    "\n",
    "class HamburgerDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        hamdata = np.loadtxt('./vectors.csv',\n",
    "            delimiter=',', dtype=np.float32)\n",
    "        self.len = hamdata.shape[0]\n",
    "        self.info_data = torch.from_numpy(hamdata[:, 0:15])\n",
    "        self.target_data = torch.from_numpy(hamdata[:, 15:])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.info_data[index], self.target_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        hamdata = np.loadtxt('./tester.csv',\n",
    "            delimiter=',', dtype=np.float32)\n",
    "        self.len = hamdata.shape[0]\n",
    "        self.info_data = torch.from_numpy(hamdata[:, 0:15])\n",
    "        self.target_data = torch.from_numpy(hamdata[:, 15:])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.info_data[index], self.target_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "dataset = HamburgerDataset()\n",
    "testset = TestDataset()\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=num_workers)\n",
    "\n",
    "test_loader =  DataLoader(dataset=dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a Batch of Training Data\n",
    "\n",
    "The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View an Image in More Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Network [Architecture](http://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "The architecture will be responsible for seeing as input a 784-dim Tensor of pixel values for each image, and producing a Tensor of length 10 (our number of classes) that indicates the class scores for an input image. This particular example uses two hidden layers and dropout to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=15, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer (32)    2개의 히든 레이어, 각각 32개의 노드\n",
    "        hidden_1 = 32\n",
    "        hidden_2 = 32\n",
    "        # linear layer (15 -> hidden_1)     15개의 데이터 (연령6, 성별1, 고기종류3, 브랜드4, 매운거선호1)\n",
    "        self.fc1 = nn.Linear(15, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)       일단 10개의 아웃풋\n",
    "        self.fc3 = nn.Linear(hidden_2, 10)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data    일단 0.2의 드롭아웃, 0.8만큼 학습\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        # x = x.view(-1, 28 * 28)   어차피 1차원 이므로 flaten 할 필요 없음\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "       # x = F.relu(self.fc3(x))\n",
    "        # softmax를 적용하여 반환한다.  => 일단 취소\n",
    "        return self.fc3(x)\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "It's recommended that you use cross-entropy loss for classification. If you look at the documentation (linked above), you can see that PyTorch's cross entropy function applies a softmax funtion to the output layer *and* then calculates the log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "# criterion = nn.CrossEntropyLoss() # crossEntropyLoss는 2차원 인풋만 된다고 해서 다른 loss function 을 사용할 것임\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the Network\n",
    "\n",
    "The steps for training/learning from a batch of data are described in the comments below:\n",
    "1. Clear the gradients of all optimized variables\n",
    "2. Forward pass: compute predicted outputs by passing inputs to the model\n",
    "3. Calculate the loss\n",
    "4. Backward pass: compute gradient of the loss with respect to model parameters\n",
    "5. Perform a single optimization step (parameter update)\n",
    "6. Update average training loss\n",
    "\n",
    "The following loop trains for 50 epochs; take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.090656\n",
      "Epoch: 2 \tTraining Loss: 0.050880\n",
      "Epoch: 3 \tTraining Loss: 0.034559\n",
      "Epoch: 4 \tTraining Loss: 0.029662\n",
      "Epoch: 5 \tTraining Loss: 0.026418\n",
      "Epoch: 6 \tTraining Loss: 0.025848\n",
      "Epoch: 7 \tTraining Loss: 0.022860\n",
      "Epoch: 8 \tTraining Loss: 0.021503\n",
      "Epoch: 9 \tTraining Loss: 0.020164\n",
      "Epoch: 10 \tTraining Loss: 0.018738\n",
      "Epoch: 11 \tTraining Loss: 0.017404\n",
      "Epoch: 12 \tTraining Loss: 0.016298\n",
      "Epoch: 13 \tTraining Loss: 0.014779\n",
      "Epoch: 14 \tTraining Loss: 0.013900\n",
      "Epoch: 15 \tTraining Loss: 0.012971\n",
      "Epoch: 16 \tTraining Loss: 0.011801\n",
      "Epoch: 17 \tTraining Loss: 0.011235\n",
      "Epoch: 18 \tTraining Loss: 0.010033\n",
      "Epoch: 19 \tTraining Loss: 0.009112\n",
      "Epoch: 20 \tTraining Loss: 0.008569\n",
      "Epoch: 21 \tTraining Loss: 0.007424\n",
      "Epoch: 22 \tTraining Loss: 0.006750\n",
      "Epoch: 23 \tTraining Loss: 0.006553\n",
      "Epoch: 24 \tTraining Loss: 0.005953\n",
      "Epoch: 25 \tTraining Loss: 0.005155\n",
      "Epoch: 26 \tTraining Loss: 0.004962\n",
      "Epoch: 27 \tTraining Loss: 0.004642\n",
      "Epoch: 28 \tTraining Loss: 0.004235\n",
      "Epoch: 29 \tTraining Loss: 0.003581\n",
      "Epoch: 30 \tTraining Loss: 0.003104\n",
      "Epoch: 31 \tTraining Loss: 0.003001\n",
      "Epoch: 32 \tTraining Loss: 0.002702\n",
      "Epoch: 33 \tTraining Loss: 0.002766\n",
      "Epoch: 34 \tTraining Loss: 0.002844\n",
      "Epoch: 35 \tTraining Loss: 0.002165\n",
      "Epoch: 36 \tTraining Loss: 0.002161\n",
      "Epoch: 37 \tTraining Loss: 0.002240\n",
      "Epoch: 38 \tTraining Loss: 0.002235\n",
      "Epoch: 39 \tTraining Loss: 0.001974\n",
      "Epoch: 40 \tTraining Loss: 0.001838\n",
      "Epoch: 41 \tTraining Loss: 0.001856\n",
      "Epoch: 42 \tTraining Loss: 0.001678\n",
      "Epoch: 43 \tTraining Loss: 0.001610\n",
      "Epoch: 44 \tTraining Loss: 0.001847\n",
      "Epoch: 45 \tTraining Loss: 0.001576\n",
      "Epoch: 46 \tTraining Loss: 0.001508\n",
      "Epoch: 47 \tTraining Loss: 0.001457\n",
      "Epoch: 48 \tTraining Loss: 0.001540\n",
      "Epoch: 49 \tTraining Loss: 0.001238\n",
      "Epoch: 50 \tTraining Loss: 0.001353\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def train(n_epochs):\n",
    "    model.train() # prep model for training\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        for data, target in train_loader:\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # print training statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch+1, \n",
    "            train_loss\n",
    "            )) \n",
    "        \n",
    "train(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network\n",
    "\n",
    "Finally, we test our best model on previously unseen **test data** and evaluate it's performance. Testing on unseen data is a good way to check that our model generalizes well. It may also be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its overall loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 2.0435807704925537500 (0%)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "for data, target in test_loader:\n",
    "    output = model(data)\n",
    "    test_loss += criterion(output,target).data\n",
    "    pred = output\n",
    "\n",
    "    correct += abs(pred.sub(target.data.view_as(pred))).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}{} ({:.0f}%)'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Test Results\n",
    "\n",
    "This cell displays test images and their labels in this format: `predicted (ground-truth)`. The text will be green for accurately classified examples and red for incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-48bf1d79b703>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# plot the images in the batch, along with predicted and true labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds = torch.max(output, 1)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
